{"nbformat_minor": 2, "cells": [{"execution_count": 106, "cell_type": "code", "source": "# all paths\npatientsRecordsPath = \"wasb:///hive_patient_master_dd-1lab-20171016-140000.csv\"\nlabORdersResultPath =  \"wasb:///hive_accession_orders_results-1lab-20171019-104500.csv\"\ndiagnosisRecordsPath = \"wasb:///hive_diagnosis_dd-1lab-20171016-140000.csv\"\ncptRecordsPath = \"wasb:///hive_cpts_dd-1lab-20171016-140000.csv\"\noutputPath = \"wasb:///patient-ranking\"\ndemographicsPath = \"wasb:///demographics/all-patients.txt\"\n", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 96, "cell_type": "code", "source": "# Search By Test Name and Value\ndef searchTestRecords(row, testName, testExpression, isFasting):\n    testRecord = row[0]\n    actualtestValue = testRecord.result_idw_result_value.isdigit() and int(testRecord.result_idw_result_value)\n    testEvalString = testExpression.format(actualtestValue)\n    isFastingMatch =  testRecord.accession_idw_fasting_ind == 'Y' if isFasting else True\n    testValueMatch = actualtestValue and eval(testEvalString) and isFastingMatch\n    actualTestName = testRecord.result_idw_result_name\n    return ((testName in actualTestName) and testValueMatch)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 97, "cell_type": "code", "source": "def handleResource(op, records):\n    resource = op['resource']\n    resourceType = resource['resourceType']\n    \n    if resourceType == 'Observation':\n        records = records.filter(lambda x : searchTestRecords(x, op['resource']['code']['text'],op['resource']['expression'], True))\n    \n    return records\n\ndef handleAllOperators(operators, records):\n    for op in operators :\n        if('resource') in op:\n            print('and (')\n            records = records.intersection(handleResource(op, records))\n            print(')')\n\n        if('any') in op:\n            print('and (') \n            records = records.intersection(handleAnyOperators(op['any'], records))\n            print(')') \n\n        if('and') in op:\n            print('and (') \n            records = records.intersection(handleAllOperators(op['and']),records )\n            print(')') \n    \n    return records\n        \ndef handleAnyOperators(operators, records):\n    \n    orrecords = False\n    for op in operators :\n        if('resource') in op:\n            print('or (')\n            if (not orrecords):\n                orrecords = True\n                records = handleResource(op, records)\n            records = records.union (handleResource(op, records))\n            print(')')\n\n        if('any') in op:\n            print('or (')\n            if (not orrecords):\n                orrecords = True\n                records = handleAnyOperators(op['any'],records)\n            records = records.union (handleAnyOperators(op['any'],records))\n            print(')')\n\n        if('all') in op:\n            print('or (')\n            records = records.union (handleAllOperators(op['and'],records))        \n            print(')')\n            \n    return records\n            ", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 107, "cell_type": "code", "source": "# read the file\nlabData = sc.textFile(labORdersResultPath)\npatientData = sc.textFile(patientsRecordsPath)\ndiagnosisRecords = sc.textFile(diagnosisRecordsPath)\ncptRecords = sc.textFile(cptRecordsPath)\ndemographics = sc.textFile(demographicsPath)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 3, "cell_type": "code", "source": "# Filter the header row \npatientHeader = patientData.first()\nlabHeader = labData.first()\ndiagnosisHeader = diagnosisRecords.first()\ncptHeader = cptRecords.first()", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 4, "cell_type": "code", "source": "# Get the data row\npatientDataWoHeader = patientData.filter(lambda x: x !=patientHeader)\nlabDataWoHeader = labData.filter(lambda x: x !=labHeader) \ndiagnosisRecordWoHeader = diagnosisRecords.filter(lambda x: x !=diagnosisHeader) \ncptRecordWoHeader = cptRecords.filter(lambda x: x !=cptHeader) ", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 5, "cell_type": "code", "source": "import csv\nfrom io import StringIO\nfrom collections import namedtuple", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 6, "cell_type": "code", "source": "patientFields   = patientHeader.replace(\" \",\"_\").replace(\"/\",\"_\").split(\"\\t\")\nlabFields   = labHeader.replace(\" \",\"_\").replace(\"/\",\"_\").split(\"\\t\")\ndiagnosisRecordsFields = diagnosisHeader.split(\"\\t\")\ncptRecordsFields = cptHeader.split(\"\\t\")\npatientDemoGraphicsFields = ['']", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 7, "cell_type": "code", "source": "cptRecordsFields", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['unique_accession_id', 'idw_accn_id', 'idw_cpt_id', 'idw_service_id', 'idw_dos_id', 'idw_analyte_code', 'idw_implosion_level', 'idw_cpt_level', 'idw_quantity', 'idw_qbs_bill_number', 'idw_date_created', 'idw_date_modified', 'idw_cpt_code', 'idw_cpt_name', 'idw_qbs_service_code', 'idw_qbs_service_name', 'dummy_cpts_ind', 'idw_lab_code']"}], "metadata": {"collapsed": false}}, {"execution_count": 8, "cell_type": "code", "source": "Patient   = namedtuple('Patient', patientFields, verbose=False)\nLabRecord   = namedtuple('LabRecord', labFields, verbose=False)\nDiagnosisRecord = namedtuple ('DiagnosisRecord', diagnosisRecordsFields, verbose = False)\nCptRecord = namedtuple ('CptRecord', cptRecordsFields, verbose = False)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 9, "cell_type": "code", "source": "def parseLabRecord(row):\n    reader = csv.reader(StringIO(row), delimiter='\\t')\n    row=next(reader)\n    return LabRecord(*row)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 10, "cell_type": "code", "source": "def parsePatient(row):\n    reader = csv.reader(StringIO(row), delimiter='\\t')\n    row=next(reader)\n    return Patient(*row)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 11, "cell_type": "code", "source": "def parseDiagnosisRecord(row):\n    reader = csv.reader(StringIO(row), delimiter='\\t')\n    row=next(reader)\n    return DiagnosisRecord(*row)  ", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 12, "cell_type": "code", "source": "def parseCptRecord(row):\n    reader = csv.reader(StringIO(row), delimiter='\\t')\n    row=next(reader)\n    return CptRecord(*row)           ", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 13, "cell_type": "code", "source": "# filter record which do not have all the columns\nlabDataWoHeaderFiltered = labDataWoHeader.filter(lambda x: len(x.split(\"\\t\")) == len(labFields) )\n", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 14, "cell_type": "code", "source": "# map the records\npatients = patientDataWoHeader.map(parsePatient)\nlabRecords = labDataWoHeaderFiltered.map(parseLabRecord)\ndiagnosis = diagnosisRecordWoHeader.map(parseDiagnosisRecord)\ncpts = cptRecordWoHeader.map(parseCptRecord)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 15, "cell_type": "code", "source": "# Search By Test Name and Value\ndef searchTestAndValue(row, testData):\n    testValue = testData['value']\n    testName = testData['name']\n    isFasting = testData['fasting']\n    actualtestValue = row.result_idw_result_value.isdigit() and int(row.result_idw_result_value)\n    testEvalString = testValue.format(actualtestValue)\n    isFastingMatch =  row.accession_idw_fasting_ind == 'Y' if isFasting else True\n    testValueMatch = actualtestValue and eval(testEvalString) and isFastingMatch\n    actualTestName = row.result_idw_result_name\n    return ((testName in actualTestName) and testValueMatch)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 16, "cell_type": "code", "source": "# Search By Patient Demographics\ndef searchPatientByDemoGraphics(row, patientDat):\n    testAge = patientDat['age']\n    testGender = patientDat['gender']\n    ageInYears = int(row.age_in_years) if row.age_in_years.isdigit() else 0 \n    ageEvalString = testAge.format(ageInYears)\n    genderEvalString = testGender.format(row.pm_gender)\n    return eval(ageEvalString) and eval(genderEvalString)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 17, "cell_type": "code", "source": "testMatchFields = ['patientId','criteria','testNameMatch', 'testValueMatch']\nTestMatch = namedtuple('TestMatch', testMatchFields, verbose=False)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 18, "cell_type": "code", "source": "def rankPatientsByTest(row, testData):\n    testValue = testData['value']\n    testName = testData['name']\n    isFasting = testData['fasting']\n    patient = row[0]\n    testRecord = row[1]\n    actualtestValue = testRecord.result_idw_result_value.isdigit() and int(testRecord.result_idw_result_value)\n    testEvalString = testValue.format(actualtestValue)\n    isFastingMatch =  testRecord.accession_idw_fasting_ind == 'Y' if isFasting else True\n    actualTestName = testRecord.result_idw_result_name\n    testNameMatch = (testName in actualTestName)\n    testValueMatch = testNameMatch and actualtestValue and eval(testEvalString) and isFastingMatch    \n    patientId = patient.pat_master_id\n    return TestMatch(patientId = patientId,criteria = testValue.format(testName), testNameMatch = testNameMatch, testValueMatch = testValueMatch)\n    \n", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 19, "cell_type": "code", "source": "criteria = {\n    'test':{\n     'name': 'LDL-C',\n     'value': '{0} > 130 and {0} < 190',\n     'fasting': True\n  },\n    'demographics': {\n        'age':' {0} > 45',\n        'gender':'\\'{0}\\' == \\'M\\''\n    }    \n}\n", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 98, "cell_type": "code", "source": "#input criteria\nimport json\n\ndef loadCriteriaJson(trialId):\n    inputCriteriaPath =\"wasb:///input/{0}.json\".format(trialId)\n    jsonRDD = sc.wholeTextFiles(inputCriteriaPath).map(lambda x: x[1])\n    criteria = jsonRDD.collect()[0];\n    json_data = json.loads(criteria)\n    return json_data\n    ", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 100, "cell_type": "code", "source": "\ndef filterRecordsByCriteria(trialId):\n\n    json_data = loadCriteriaJson(trialId)\n    \n    labRecordsMap = labRecords.map(lambda x :(x.pat_master_id, x))\n    patientsMap = patients.map(lambda x :(x.pat_master_id, x))\n    \n    allRecords = labRecordsMap.join(patientsMap).map(lambda x: x[1])  \n    \n    if('any') in json_data:\n        operators = json_data['any']\n        filteredRecords = handleAnyOperators(operators, allRecords)\n\n    if('all') in json_data:\n        operators = json_data['all']\n        filteredRecords = handleAllOperators(operators, allRecords)\n\n    if('resource') in json_data:\n        resource = json_data['resource']\n        filteredRecords = handleResource(resource, allRecords)\n\n    return filteredRecords\n\n", "outputs": [], "metadata": {"scrolled": true, "collapsed": false}}, {"execution_count": 103, "cell_type": "code", "source": "filteredRecords = filterRecordsByCriteria('12346')\n\n", "outputs": [{"output_type": "stream", "name": "stderr", "text": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 81.0 failed 4 times, most recent failure: Lost task 0.3 in stage 81.0 (TID 166, 10.0.0.4, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 171, in main\n    process()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 166, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 1338, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<stdin>\", line 3, in <lambda>\nAttributeError: 'tuple' object has no attribute 'pat_master_id'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1928)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1941)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1954)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor95.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 171, in main\n    process()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 166, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 1338, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<stdin>\", line 3, in <lambda>\nAttributeError: 'tuple' object has no attribute 'pat_master_id'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\nTraceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 1342, in take\n    res = self.context.runJob(self, takeUpToNumLeft, p)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/context.py\", line 978, in runJob\n    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 81.0 failed 4 times, most recent failure: Lost task 0.3 in stage 81.0 (TID 166, 10.0.0.4, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 171, in main\n    process()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 166, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 1338, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<stdin>\", line 3, in <lambda>\nAttributeError: 'tuple' object has no attribute 'pat_master_id'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1928)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1941)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1954)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor95.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 171, in main\n    process()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 166, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 1338, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<stdin>\", line 3, in <lambda>\nAttributeError: 'tuple' object has no attribute 'pat_master_id'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n\n"}], "metadata": {"collapsed": false}}, {"execution_count": 108, "cell_type": "code", "source": "filteredRecordsMap = filteredRecords.map(lambda x:  (x[0].pat_master_id, x))\ndemographicsMap = demographics.map(lambda x:  (x[0], x))\nfilteredRecords.join(demographics).map(lambda x: x[1]).take(1)", "outputs": [{"output_type": "stream", "name": "stderr", "text": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 87.0 failed 4 times, most recent failure: Lost task 9.3 in stage 87.0 (TID 197, 10.0.0.5, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 171, in main\n    process()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 166, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<stdin>\", line 6, in <lambda>\n  File \"<stdin>\", line 5, in searchTestRecords\nKeyError: '0 '\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1928)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1941)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1954)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor95.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 171, in main\n    process()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 166, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<stdin>\", line 6, in <lambda>\n  File \"<stdin>\", line 5, in searchTestRecords\nKeyError: '0 '\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\nTraceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 1342, in take\n    res = self.context.runJob(self, takeUpToNumLeft, p)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/context.py\", line 978, in runJob\n    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 87.0 failed 4 times, most recent failure: Lost task 9.3 in stage 87.0 (TID 197, 10.0.0.5, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 171, in main\n    process()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 166, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<stdin>\", line 6, in <lambda>\n  File \"<stdin>\", line 5, in searchTestRecords\nKeyError: '0 '\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1928)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1941)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1954)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor95.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 171, in main\n    process()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 166, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<stdin>\", line 6, in <lambda>\n  File \"<stdin>\", line 5, in searchTestRecords\nKeyError: '0 '\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n\n"}], "metadata": {"collapsed": false}}, {"execution_count": 20, "cell_type": "code", "source": "# filter by the criteria\nfilteredTests = labRecords.filter(lambda x: searchTestAndValue(x, criteria['test']))\nfilteredPatients = patients.filter(lambda x : searchPatientByDemoGraphics(x, criteria['demographics']))\ntestCount = filteredTests.count()\npatientsCount = filteredPatients.count()\ntotalPatCount = patients.count()\ntotalRecordsCount = labRecords.count()\npatientsCount\ntotalPatCount\ntotalRecordsCount", "outputs": [{"output_type": "stream", "name": "stdout", "text": "1754327"}], "metadata": {"collapsed": false}}, {"execution_count": 21, "cell_type": "code", "source": "# join the lab records and patients\nfilteredTestsmap = filteredTests.map(lambda x :(x.pat_master_id, x))\nfilteredpatientsmap = filteredPatients.map(lambda x :(x.pat_master_id, x))\njoinResult = filteredTestsmap.join(filteredpatientsmap).map(lambda x: x[1])\nresCount = joinResult.map(lambda x: \"{} {} {}\".format(x[0].result_idw_result_name, x[0].result_idw_result_value,x[1].age_in_years)).count()\nresCount", "outputs": [{"output_type": "stream", "name": "stdout", "text": "24"}], "metadata": {"collapsed": false}}, {"execution_count": 22, "cell_type": "code", "source": "# Ranking of patients by test matching\nallTestsMap = labRecords.map(lambda x :(x.pat_master_id, x))\njoinResult = filteredpatientsmap.join(allTestsMap).map(lambda x: x[1])\njoinCount = joinResult.map(lambda x: rankPatientsByTest(x,  criteria['test'])).filter(lambda x : x.testNameMatch == True and x.testValueMatch == True).count()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 23, "cell_type": "code", "source": "import uuid\nuuid.uuid4()\nfileName = \"{0}-{1}\".format(outputPath,str(uuid.uuid4()))\n", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 24, "cell_type": "code", "source": "joinResult.saveAsTextFile(fileName)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}